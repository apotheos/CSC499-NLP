Chapter 6 Exercise 8
===================

*Word features can be very useful for performing document classification, since
the words that appear in a document give a strong indication about what its semantic
content is. However, many words occur very infrequently, and some of the
most informative words in a document may never have occurred in our training
data. One solution is to make use of a lexicon, which describes how different words
relate to one another. Using the WordNet lexicon, augment the movie review
document classifier presented in this chapter to use features that generalize the
words that appear in a document, making it more likely that they will match words
found in the training data.*

Approach
--------

WordNet has the concept of synsets, or sets of synonyms for given words.
One word may be a part of several synsets (if it's spelled the same but
used in different senses, or used as a different part of speech, for example),
and a synset may have many words.

Since synsets are made up of words, it can be said that a synset is more general
than a word, fulfilling the generalization part of the exercise.

To implement this into the classifier given earlier in the section, I removed the
"contains(<word>)" feature and replaced it with a "Synset(<synset>)" feature.
Because of the framework already constructed for me by this example, it was considerably
difficult to use POS context to get the synsets (`wordnet.synsets(word, tag)`). Instead,
I found the synsets without part of speech context (`wordnet.synsets(word)`).

Results
-------

Here is the a run of the original, unmodified, example classifier:

```
Accuracy: 0.81

Most Informative Features
    contains(outstanding) = True    pos : neg   =   11.1 : 1.0
    contains(seagal) = True         neg : pos   =    7.7 : 1.0
    contains(wonderfully) = True    pos : neg   =    6.8 : 1.0
    contains(damon) = True          pos : neg   =    5.9 : 1.0
    contains(wasted) = True         neg : pos   =    5.8 : 1.0
```

And here is a run through the modified synset-based classifier:

```
Accuracy: 0.74

Most Informative Features
Synset('outstanding.s.01') = True   pos : neg    =     11.1 : 1.0
Synset('outstanding.s.03') = True   pos : neg    =      8.4 : 1.0
Synset('feeble.s.01') = True        neg : pos    =      5.9 : 1.0
Synset('damon.n.01') = True         pos : neg    =      5.7 : 1.0
Synset('lame.n.02') = True          neg : pos    =      5.4 : 1.0
Synset('squandered.s.01') = True    neg : pos    =      5.2 : 1.0
Synset('farcical.s.01') = True      neg : pos    =      5.1 : 1.0
Synset('waste.v.08') = True         neg : pos    =      5.0 : 1.0
Synset('waste.v.04') = True         neg : pos    =      5.0 : 1.0
Synset('waste.v.03') = True         neg : pos    =      5.0 : 1.0
```

Analysis
--------

The modified classifier performed ~9% worse than the original. Why? Because
there was no POS context available, synsets that were not actually valid made
it into the feature set. A word is only ever used in one sense at a time
(except in rare cases like double entendres). A word, however, might have
many uses. Since there was no part of speech tag to help reduce the resulting
synsets, all were returned and all were included in the feature set.

This has the effect of double counting and false positives on the feature set,
thereby lowering its quality. It's likely that a POS preprocessor would improve
results.
