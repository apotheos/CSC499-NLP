Chapter 5 Problem 37
====================

The Problem
-----------

Our approach for tagging an unknown word has been to consider the letters of the word (using `RegExpTagger()`), or to ignore the word altogether and tag it as a noun (using `nltk.DefaultTagger()`). These methods will not do well for texts having new words that are not nouns. Consider the sentence *I like to blog on Kim's blog*. If *blog* is a new word, the looking at the previous tag (`TO` versus `NP$`) would probably be helpful, i.e., we need a default tagger that is sensitive to the preceding tag.
1. Create a new kind of unigram tagger that looks at the tag of the previous word, and ignores the current word. (The best way to do this is to modify the source code for `UnigramTagger()`, which presumes knowledge of object-oriented programming in Python.)
2. Add this tagger to the sequence of backoff taggers (including ordinary trigram and bigram taggers that look at words), right before the usual default tagger.
3. Evalutate the contribution of this new tagger.

Source code for the solution is located in `ch_5/exercises/improved_tagger.py`.

The Approach
------------

After careful reading of the source code for UnigramTagger and most of its superclasses, I discovered that
creating a tagger that looks at the preceding word is a relatively simple task. Learning that, though, was
a relatively difficult task as I had to make sure overriding of the correct method wouldn't cause problems
elsewhere in the library and would produce the desired effect.

The Implementation
------------------

After discovering how to get the behavior I want from the UnigramTagger, implementing the change was
relatively easy:

```python
class PreviousUnigramTagger(nltk.UnigramTagger):

    def context(self, tokens, index, history):
        return tokens[index - 1] if index else None
```

The context method is used by the tagger to decide how to look up the tag for a given word. For a unigram
tagger, a context is a single word. For any (n > 1)-Gram tagger, the context is a list of n words. This
overridden implementation of the `context` is used by both the training operation and the actual tagger.

The trainer goes through each word in a sentence, finds its context, and increments the tag of the current
word. In the ordinary UnigramTagger, the context is the same as the current word, meaning the tag applies
directly to that word. In this modified version of the unigram tagger, the trainer increments the tag of
the current word in the entry for the previous word in the frequency distribution it uses.

The tagging process works much the same way. Given a sentence, the tagger will attempt to find the current
word's context. The modified `context` method returns the previous word, looking up the new tag by an
already known word rather than the previously unknown word.

The Results
-----------

To test this new tagger, I constructed two different taggers, one that does not use the new tagger and one
that does. I then tested them against the brown corpus's news genre and got the following results:

```
Original: 0.842320342869
Modified: 0.833947971693
```

These are surprising results. It means that the addition of the new "previous word context" backoff tagger
made the results worse. After spending a while thinking about why this might be and where my code was
breaking, I realized it was not my code at all. The problem states that the original tagger, "will not do
well for texts having new words that are not nouns".

It's likely that the unknown words in the 'news' corpus are overwhelmingly nouns, even when their preceding
word might suggest otherwise. For example, the word "to"---tagged with the `TO` tag---is most likely to be
followed by a verb (We are going **to eat**). However, the word "to" is also used in circumstances that do
not have it follow with a verb (We are going **to Canada**). Even though it's more likely for a verb to
follow the word "to", it's probable that most *unknown* words following "to" are more likely to be nouns
rather than verbs. This modified tagger does not account for that. Perhaps, as the problem said, a text
wherein new words are more likely not to be nouns than the in 'news' corpus might be better suited to this
approach.