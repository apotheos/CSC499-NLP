Problem 24
==========

Part A
------

See the `generate_model` function in `ch_2/exercises/24.md`.

Part B
------

I decided to have fun with the `PlaintextCorpusReader`, so instead of picking one of the "genres" that come with the
NLTK library, I used a collection of two texts from Sherlock Holmes: *The Adventures of Sherlock Holmes* and
*A Study in Scarlet*, all from Project Gutenberg. Here are examples from a few runs:

**Using the most likely candidate word all of the time**

Start word = "elementary":

> elementary , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door ,

Start word = "gun":

> gun and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and the door , and

Clearly, this approach is not going to work because the most common thing to come after "and" is "door" and the most likely thing to come after "door" is "and", leading to an infinite loop.
I then implemented the improvements suggested by the problem, and got the output below.

**Randomly selecting one of the candidate words**

Start word = "elementary":

> elementary problems help me . Chubb lock this Miss Turner made shorthand account lose not write to conceive . Round one limb , than thirty . Snapping away I in producing a playful smile upon what mother is dazed face clouded over their singular story ." Leaning back both from

Start word = "gun":

> gun and thrusting this piece had my reasoning . Naturally , ostlers , completed his bird all does ; at scratch and as they stole over yonder . Vincent Spaulding ?" remarked Sherlock Holmes welcomed her back in dreadful hours if on presenting it always appears that ever see now

This is much better. The dreaded door loop doesn't even make an appearance, and the sentences are almost intelligible.
Obviously, this is one of the benefits of this message. Another benefit is the simplicity of the algorithm, making it
very little work for the computer to implement this solution. Finally, it gives more variety to the results, because
selecting a random word allows for more surprises and possible outcomes in the output.

There are also disadvantages to this method. Namely, the variety that was an advantage above is also a disadvantage.
Specifically, selecting a random word from the text is not realistic, since it's more likely for a real human to pick
words as a function of how likely he or she is to use that word after another.

In the end, both solutions are artificial and at two extremes. Where the first allowed for only one path from one word
to another, the second led to a large number of paths while disregarding the *frequency* portion of the frequency
distribution. A compromise might use a mathematical function that selects the next word based on random selection
according to the frequency of occurrence. That is, make it more likely (but nowhere near guaranteed) for the `.max()`
word to be selected, while still making it possible (but very unlikely) that the least likely (or `.min()`) word is
selected. **How might we define a function that does this?**

Below I have created a better, but still naive solution that tries to compromise the two extremes above without
involving much math. Instead selecting randomly from the set of all possible outcomes, we select randomly from
the five (at most) likely outcomes. That is:

```python
word = random.choice(list(cfdist[word])[:5])
```

**Randomly selecting one of the top three candidate words**

Start word = "elementary":

> elementary problems ." " Yes . He is a small estate of it is a little more to me ." The Adventure of it was not know what it is not have had the room . I have the house at my own family ." He had not a small

Start word = "gun":

> gun as to his head of it , I am afraid of it is not a very good - night . He had a very bad ?" asked , with my hand . The Coroner : " And yet , with my friend and the house in his hand over

Of the three options, this seems go generate the most intelligible text, but it is still possible to do it better.


Part C
------

I've chosen to combine the Sherlock Holmes corpus with custom Pokemon-themed corpus using the text dumps from *Pokemon
Red* and *Pokemon Crystal*, along with the script to *Pokemon: the Movie*. Here are a few examples of the output:

*Start word = "Pikachu"*

> Pikachu . It must have the matter , and that it . I had a little of it is no doubt upon his head . The other people , the door of it , but he was a little problem which I am the room and that he . It

*Start word = "Squirtle"*

> Squirtle ' m sorry . It is a few years , but the other people who has been so , but it , but the door was no one to me . I have the matter ?" I had been in this morning ." I had been so that I

These are interesting results, because all three examples clearly cross over into both genres of text. In the two
*Start word = "Sherlock"*

> Sherlock Holmes , you ' m going on ! i was not have a pokémon are , I have been a pokémon are in a pokémon . The name : you . i don kametukusu saidon sandopan parasekuto goro - night . i don ' re the matter to see

Pokemon-seeded examples, both cross over from the Pokemon genre into the Sherlock genre. We can tell this because of
the "door" and surrounding words. "Door" must have been a crossover word for Pokemon to Sherlock, since it is usually
preceeded by a common word.

On the other side of things, the Sherlock example goes into the Pokemon world (and later on into who-knows-where) rather
quickly, happening after the word "a", making the article a crossover word between the two corpora.

Initially, it surprised me that there was so much crossover, but since this crossover happens mostly on stopwords,
it makes sense after thinking about it.

